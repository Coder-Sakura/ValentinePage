<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[你相信这个世界能加速吗？]]></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Frang-xia-zai-su-du-de-dao-jie-fang%2F</url>
    <content type="text"><![CDATA[说在前面的话 本篇概述：重新整理了一下关于度盘的黑科技 整理的时候发现 度盘下载器 的作者不维护了╮(╯▽╰)╭，今天也换了多个文件试了下貌似都是HTTP响应超时字样，详见https://www.linesoft.top/archives/4/，毕竟也是陪伴多年的好朋友，献上一朵❀❀(~~绝对不是因为其他3作更好用一点的原因~~) 那么以下3作推荐给大家 加速世界——你相信这个世界能加速吗？加速通道 1加速通道 2加速通道 3proxyee-down 简介：Proxyee Down 是一款开源的免费 HTTP 高速下载器，底层使用netty开发，支持自定义 HTTP 请求下载且支持扩展功能，可以通过安装扩展实现特殊的下载需求。使用手册 预览： github地址：github 直接下载地址(建议看一遍使用手册)：地址 PanDownload 简介：百度盘第三方下载工具 预览： 地址：PanDownload 密码：co50 SpeedPan 简介：SpeedPan又名速盘，一款百度网盘满速下载工具利器 预览： 地址：SpeedPan 密码：7rn6]]></content>
      <categories>
        <category>资源分享</category>
      </categories>
      <tags>
        <tag>share</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python/Mysql基本使用]]></title>
    <url>%2Fblog%2F2019%2F04%2F22%2Fpy-mysql%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：python连接mysql的一些操作，欢迎指错 1、MySQLdbpython连接mysql用的模块。MySQLdb主要还是聚焦于如何和数据库进行连接和进行基本的操作，操作的体现形式主要还是进行SQL语句的执行。 安装 1pip3 install mysqlclient 2、基本使用1、数据库连接123456import MySQLdb# 连接数据库 ——&gt; 主机host,端口,用户,密码,数据库,编码conn = MySQLdb.connect(host='localhost',port=3306,user='你的账号',passwd='你的密码',db='你要操作的数据库',charset='utf8')# 连接成功后,创建一个游标对象用于操作数据库# 获取到游标后再进行数据库操作cur = conn.cursor() 2、查询12345678910111213141516171819202122# 查询cur.execute("SELECT VERSION()") # 这里不返回结果，只是执行version = cur.fetchall()# 返回的是元组 (('5.7.14',),)# fetchall() 是返回所有匹配的元组,接收全部的返回结果行# fetchone() 只返回一个匹配的元组,然后游标后移# 我这里用了自己数据库中的一个表演示fetchall()和fetchone()# 1. fetchall()cur.execute("SELECT * from test")a = cur.fetchall()print(a,len(a))# 这里数据量太大,结果就不放出来了# 2. fetchone()cur.execute("SELECT * from test")data = cur.fetchone()i = 0while i&lt;10: print(data) i += 1 data = cur.fetchone() 附图 3、插入12345678910111213141516171819202122232425262728# 插入# sql语句,%s占位sql = "INSERT INTO test(date,tag,title,link) VALUES(%s,%s,%s,%s)"# 1. 插入一条execute() data是元组data = ('19/02/14', 'Android', 'Android\xa0fdget()\xa0优化导致的\xa0binder\xa0UAF\xa0漏洞（CVE-2019-2000）\xa0：', 'https://bugs.chromium.org/p/project-zero/issues/detail?id=1719')try: cur.execute(sql,data) # 事务commit后才会真正插入数据 conn.commit()except Exception,e: # 出错回滚 conn.rollback()finally： cur.close() conn.close() # 断开数据库连接 # 2. 批量插入executemany()datas = [('19/02/14', 'Android', 'Android\xa0fdget()\xa0优化导致的\xa0binder\xa0UAF\xa0漏洞（CVE-2019-2000）\xa0：', 'https://bugs.chromium.org/p/project-zero/issues/detail?id=1719'),('19/02/14', 'IoTDevice', '以家庭路由为例讲解 IoT 逆向工程：', 'http://va.ler.io/myfiles/dva/iot-rev-engineering.pdf')]try: cur.executemany(sql,datas) conn.commit()except Exception,e: # 出错回滚 conn.rollback()finally: cur.close() conn.close() # 断开数据库连接 4、修改，更新12345678910# sqlsql = "UPDATE EMPLOYEE SET AGE = AGE + 1 WHERE SEX = '%c'" % ('M')try: cur.execute(sql) conn.commit()except: conn.rollback()finally: cur.close() conn.close() 5、删除12345678910# sql 删除数据,年龄大于20sql = "DELETE FROM EMPLOYEE WHERE AGE &gt; %s" % (20)try： cur.execute(sql) conn.commit()except: conn.rollback()finally: cur.close() conn.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python/Tkinter/lxml 签名生成器]]></title>
    <url>%2Fblog%2F2019%2F04%2F19%2Fsignature-generator%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：主要记录爬虫+GUI制作签名生成器 1、爬虫实现 暂时留空，本篇先解析代码 2、GUI实现 暂时留空，本篇先解析代码 3、代码github地址：https://github.com/Coder-Sakura/Signature-generator 1. uustv.py ——&gt; 爬虫模块点我可以将内容伸缩哦~ ٩(๑&gt;◡&lt;๑)۶ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import requestsimport timefrom bs4 import BeautifulSoupfrom tkinter import messageboxfrom tkinter import * # PhotoImageclass Spider(object): # 初始化,包括请求头,url,支持的字体、字号,需要生成签名的文字 def __init__(self): self.headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36', &#125; self.url = 'http://m.uustv.com/' self.values = ['个性签','连笔签','潇洒签','草体签','合体签','商务签','可爱签'] self.font = ['jfcs.ttf','qmt.ttf','bzcs.ttf','lfc.ttf','haku.ttf','zql.ttf','yqk.ttf'] self.v_f = dict(zip(self.values,self.font)) self.word = '' self.fonts = '' self.size = 60 # 接受GUI页面传来的参数,首先检查是否为空,其次是选择的字体大小 def check(self,part,enter,numberChosen,sizesChosen): self.part = part self.word = enter.get() self.fonts = numberChosen.get() # 文字 self.size = sizesChosen.get() if not enter.get(): messagebox.showinfo('提示','请输入需要生成签名的文字') else: if self.fonts in self.v_f: self.fonts = self.v_f[self.fonts] # 文字转换成字体格式名称 # messagebox.showinfo('提示',word) # messagebox.showinfo('提示',fonts) self.post_img() # 发送请求,获取图片地址,并捕获图片数据返回 def post_img(self): data = &#123; 'word': self.word, 'sizes': self.size, 'fonts': self.fonts, 'fontcolor': '#000000'&#125; html = requests.post(url=self.url,data=data,headers=self.headers) html_soup = BeautifulSoup(html.text,'lxml') img_url = self.url + html_soup.find('div',attrs=&#123;'class':'tu'&#125;).find('img')['src'] print(img_url) img_html = requests.get(url=img_url,headers=self.headers) img_html.encoding = 'utf8' self.down(img_html) # 下载图片 def down(self,img_html): f = open('&#123;&#125;.gif'.format(self.word),'wb') f.write(img_html.content) f.close() self.view_img() # 将图片展示到GUI页面 def view_img(self): image = PhotoImage(file='&#123;&#125;.gif'.format(self.word)) label2 = Label(self.part,image=image) label2.bm = image label2.grid(row=3,columnspan=2) 2. uustv_gui.py点我可以将内容伸缩哦~ ٩(๑&gt;◡&lt;๑)۶ 123456789101112131415161718192021222324252627282930313233343536373839404142434445from tkinter import *from tkinter.ttk import Comboboxfrom uustv import *class GUI(object): def __init__(self): self.window_title = '签名生成器--网络精英--2019.04.12' self.label_text = '输入签名字样' self.button_text = '点击生成' self.values = ['个性签','连笔签','潇洒签','草体签','合体签','商务签','可爱签'] self.sizes = [10,20,30,40,50,60,70] # GUI def parts(self): # 部件 part = Tk() # 封装对象 part.title(self.window_title) # 标题 part.geometry('545x330') # 窗口大小 # 定义label,文字区域,只能看不能点,设置文字，文字字体和文字大小 label1 = Label(part,text='设计字样',font=('华文行楷',20)) label1.grid() # 定义布局格式，grid()表示网格格式 # 输入框 enter = Entry(part,font=('微软雅黑',20)) enter.grid(row=0,column=1) # 字体样式选择 numberChosen = Combobox(part, width=12, state='readonly') numberChosen['values'] = self.values numberChosen.current(0) numberChosen.grid(row=1,column=0) # 字体大小选择 sizesChosen = Combobox(part, width=12, state='readonly') sizesChosen['values'] = self.sizes sizesChosen.current(5) sizesChosen.grid(row=1,column=1) # 按钮,当按下按钮时,携带参数至指定函数 button = Button(part,text='点击生成',font=('微软雅黑',20),command=lambda: Spider().check(part,enter,numberChosen,sizesChosen)) button.grid(row=2,column=0) part.mainloop()gui = GUI()gui.parts() 3. 生成软件123456789# 1.首先安装pyinstallerpip3 install pyinstaller# 2.在2个py的所在目录调出cmd，输入以下命令:pyinstaller -F -w uustv.py uustv_gui.py# 在生成的文件夹中找到dict文件夹,其下有exe文件,便是最终的签名生成器# 3.如果需要自定义软件图标,需要自己准备好适当尺寸的ico格式图片# 打包的时候使用-i xxx.ico 来指定自定义的ico图标]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>tkinter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫-pixiv关注画师作品[2]]]></title>
    <url>%2Fblog%2F2019%2F03%2F30%2Fpixiv-two%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：上篇的后续；主要是分析数据接口，拿到关注画师的所有作品的详细信息 1、分析——关注界面 关注界面(默认公开) https://www.pixiv.net/bookmark.php?type=user&amp;rest=show 这里关注画师全在公开界面，虽然非公开获取也是ok的 公开和非公开只是，https://www.pixiv.net/bookmark.php?type=user&amp;rest=show 后面分别是rest=show 和 rest=hide F12 查看 Elements ，寻找画师列表和页数 画师列表全在一个 class=members 的 div 中 画师信息在user-data中，有作者 id、主页 url、作者 name 最大页数在一个 class=_pager-complex 的 div，倒数第二个li 2、代码——bs4匹配1234# 找到最大页数max_num = attention_html_soup.find('div', attrs=&#123;'class', '_pager-complex'&#125;).find_all('li')[-2].text # 画师个人信息painter_information = attention_html_soup.find('div', attrs=&#123;'class', 'members'&#125;).find_all('div', attrs=&#123;'class', 'userdata'&#125;) 3、代码——获取关注画师的信息123456789101112131415161718192021#获取关注画师界面的信息def attention_html(self): # self.return_to = 'https://www.pixiv.net/bookmark.php?type=user&amp;rest=show&amp;p=' # p 是页数 attention_html = self.request(self.return_to) attention_html_soup = BeautifulSoup(attention_html.text, 'lxml') #获取最大页数 max_num = attention_html_soup.find('div', attrs=&#123;'class', '_pager-complex'&#125;).find_all('li')[-2].text print('最大页数为%s' % (max_num)) for num in range(1,int(max_num)+1): attention_html_url = self.return_to + str(num) # 构造每个页面的 url attention_html = self.request(attention_html_url) attention_html_soup = BeautifulSoup(attention_html.text,'lxml') painter_information = attention_html_soup.find('div', attrs=&#123;'class', 'members'&#125;).find_all('div', attrs=&#123;'class', 'userdata'&#125;) #画师个人信息 for painter in painter_information: # userdata下的a标签中的data-user_id = 作者id painter_id = painter.a['data-user_id'] # userdata下的a标签中的data-user_name = 作者name name = painter.a['data-user_name'] print('&#123;0&#125;:&#123;1&#125;'.format(name,painter_id)) print('已获取所有关注画师的作品信息！！！！') 对于文笔不好的人来说，还是上代码来的舒服 毕竟 talk is cheap, show me code ! 4、分析——画师个人主页及数据流向 这里说下，为什么不获取 user-data 里面的 href 属性？ 正常流程不应该是拿到 href 然后访问 url,获取源码，拿到作品信息。 因为可以直接通过接口拿到这个画师的所有作品的数据。 该画师有73个作品，但是点击进入主页发现只有小小的一部分 勾上 Preserve log，点击查看全部 新加载的页面显示了所有的作品（虽然分为2页） 与之前的 XHR 比较，接下来的目标在红框标出来的三个文件（不一定全是我们的目标） 点击第一个illust，发现 Preview 里是标签 tags （json数据） 点击第二个illust，Preview 里是作品信息 （json数据） 确定了第二个 illust 是目标之后，模仿它进行请求 但是发现他的 url 是一串巨长的字符串，由 一页的所有作品 id 和 is_manga_top=0 拼接而成 url 中作品 id 的拼接顺序是由新到旧（也就是数字大的在前面） 1url = https://www.pixiv.net/ajax/user/1117751/profile/illusts?ids%5B%5D=73742388&amp;ids%5B%5D=71855085&amp;ids%5B%5D=71849582&amp;ids%5B%5D=71685985&amp;ids%5B%5D=68213121&amp;ids%5B%5D=67765964&amp;ids%5B%5D=67758280&amp;ids%5B%5D=67757922&amp;ids%5B%5D=67619936&amp;ids%5B%5D=67404010&amp;ids%5B%5D=66856979&amp;ids%5B%5D=65998170&amp;ids%5B%5D=65834643&amp;ids%5B%5D=65393332&amp;ids%5B%5D=63861794&amp;ids%5B%5D=63761535&amp;ids%5B%5D=63617575&amp;ids%5B%5D=63475838&amp;ids%5B%5D=63090307&amp;ids%5B%5D=62347061&amp;ids%5B%5D=62200016&amp;ids%5B%5D=62178209&amp;ids%5B%5D=61785521&amp;ids%5B%5D=61656195&amp;ids%5B%5D=61489588&amp;ids%5B%5D=60732958&amp;ids%5B%5D=60588424&amp;ids%5B%5D=60384884&amp;ids%5B%5D=59959669&amp;ids%5B%5D=59706889&amp;ids%5B%5D=59656129&amp;ids%5B%5D=59541311&amp;ids%5B%5D=59180046&amp;ids%5B%5D=58977788&amp;ids%5B%5D=58919004&amp;ids%5B%5D=58029173&amp;ids%5B%5D=57027442&amp;ids%5B%5D=57027347&amp;ids%5B%5D=56527887&amp;ids%5B%5D=56525716&amp;ids%5B%5D=56309403&amp;ids%5B%5D=56110382&amp;ids%5B%5D=55934890&amp;ids%5B%5D=55680445&amp;ids%5B%5D=54917440&amp;ids%5B%5D=54900477&amp;ids%5B%5D=54900429&amp;ids%5B%5D=54900380&amp;is_manga_top=0 下一步目标：既然有所有作品 id 拼接成的 id ，说明是有接口获取所有作品 id 的。 5、模仿请求 在 XHR 中继续寻找，发现一个叫 all 文件，点开 Preview （json数据） all 的 Request URL: https://www.pixiv.net/ajax/user/1117751/profile/all ，/user/后面的数字是作者的 id 在 Preview 中发现 illusts (插画)属性 ，下面的应该就是作品 id 了，可以自己复制一个去验证一下。 补充：有些画师在 manga (漫画)属性也是有值的，所以这里需要和前面的 illusts 属性合并 Request URL 放到浏览器中去访问，将访问结果复制到 json.cn 进行格式化 6、代码——获取关注画师的所有作品信息 获得关注画师的信息，比如 id、name 通过 https://www.pixiv.net/ajax/user/[画师id]/profile/all 来获取画师的所有作品 id （单图 动图 多图 都在 illusts 属性中，漫画虽然是单图和多图，但在 manga属性 中） 接着通过构造作品 id 和 is_manga_top=0 的 url 去请求作品的详细信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 获取关注画师界面的信息def attention_html(self): # self.return_to = 'https://www.pixiv.net/bookmark.php?type=user&amp;rest=show&amp;p=' # p 是页数 attention_html = self.request(self.return_to) attention_html_soup = BeautifulSoup(attention_html.text, 'lxml') # 获取最大页数 max_num = attention_html_soup.find('div', attrs=&#123;'class', '_pager-complex'&#125;).find_all('li')[-2].text print('最大页数为%s' % (max_num)) for num in range(1,int(max_num)+1): attention_html_url = self.return_to + str(num) # 构造每个页面的 url attention_html = self.request(attention_html_url) attention_html_soup = BeautifulSoup(attention_html.text,'lxml') painter_information = attention_html_soup.find('div', attrs=&#123;'class', 'members'&#125;).find_all('div', attrs=&#123;'class', 'userdata'&#125;) #画师个人信息 for painter in painter_information: # userdata下的a标签中的data-user_id = 作者id painter_id = painter.a['data-user_id'] # userdata下的a标签中的data-user_name = 作者name name = painter.a['data-user_name'] print('&#123;0&#125;:&#123;1&#125;'.format(name,painter_id)) # 构造url来获取作者的所有作品 id ajax_url = 'https://www.pixiv.net/ajax/user/&#123;0&#125;/profile/all'.format(painter_id) self.painter_picture(painter_id,ajax_url,name) print('获取所有关注画师信息完成！！！！') def painter_picture(self,painter_id,ajax_url,name): ajax_html = self.request(ajax_url) # 使用json.loads()方法加载进来 ajax_json = json.loads(ajax_html.text) ajax_illusts = ajax_json["body"]["illusts"] ajax_manga = ajax_json["body"]["manga"] # 判断是否有 manga 类型的作品 if len(ajax_manga) == 0: total_data_dict = dict(ajax_illusts) else: # 合并 manga 和 illusts,并转换为字典 total_data_dict = dict(ajax_illusts, **ajax_manga) # 字典格式：id:None,所以取字典的keys，并转化为list total_data = list(total_data_dict.keys()) # 这里用的是冒泡排序（从小到大），刚好学到就用了 # 其实更简单的是 list(set(total_data))[::-1] # 上面的代码得到的也是从大到小排序的作品id for x in range(len_total-1): for y in range(len_total-1-x): if total_data[y] &gt; total_data[y+1]: total_data[y],total_data[y+1] = total_data[y+1],total_data[y] # 从大到小排序的作品id total_data = total_data[::-1] # 按每48个分组,画师每页显示48个作品 limit_num = 48 # after_grouping_list = [[xxx,xxx,xxx],[48个id],[...]...] after_grouping_list = [total_data[i:i+limit_num] for i in range(0,len(total_data),limit_num)] print('画师',name,'作品有：',len(after_grouping_list),'页') # 开始根据作品id来拼接url（就是那个一大串的url） count = 0 # 每拼接完48个+1 for grouping_list in after_grouping_list: ids_big = 'https://www.pixiv.net/ajax/user/&#123;&#125;/profile/illusts?'.format(painter_id) for work_id in grouping_list: ids = 'ids%5B%5D=' + work_id + '&amp;' ids_big = ids_big + ids works_url = ids_big + 'is_manga_top=0' count += 1 print('第%s页的works_url:%s' % (count,works_url)) # 发起请求获得第count页作品的详细信息 works_html = self.request(works_url) works_json = json.loads(works_html.text) works_data = works_json["body"]["works"].values() for x in works_data: title = x['title'] folder_id = x['id'] tags = x['tags'] small_url = x['url'] pageCount = x['pageCount'] print('\n作品id:&#123;0&#125;\t作品页数:&#123;1&#125;'.format(folder_id,pageCount)) print('作品标题:&#123;0&#125;'.format(title)) print('作品标签:&#123;0&#125;'.format(tags)) print('作品250*250图片地址:&#123;0&#125;'.format(small_url)) works_data = works_json[“body”][“works”].values() 因为 keys() 是作品 id，values() 里面也有，所以直接 values() 就好了 7、最后 本篇主要是分析数据接口，拿到关注画师的所有作品的详细信息 那么下篇再根据单图动图多图进行图片下载，预计文件存储、其他的小功能和最后的代码汇总得放在下下篇了。 へ(￣ ￣;へ)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>lxml</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫-pixiv关注画师作品[1]]]></title>
    <url>%2Fblog%2F2019%2F03%2F27%2Fpixiv-one%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：大概有几篇是关于pixiv关注画师的作品抓取的思路和代码，后面大概还会有个个人收藏的抓取（这个比较简单） 1、pixiv介绍 Pixiv 这是一个墙外的网站，需要正确的扶梯姿势和科学上网 当然也可以参考我的方法（nginx + 修改 hosts 文件） 地址：点击前往 密码:3235 (建议 hosts 文件取自己需要的那部分) 为什么选 pixiv 呢？其实在12月份的时候我刚开始学 python，这个 pixiv 的小项目是我自己突发奇想要做的，那时候是新年前一周左右，对于那时候的我来说，pixiv 反爬难度一般般，但比较难的数据接口分析、构造url、文件操作、代理、图片合成等（自己一个人盯了这个网站几天，最后完成超级兴奋！） 2、Target 登录账号关注的画师的作品 思路： 首先是模拟登录（PC 用过 pixiv 的同学都知道在未登录的时候 pixiv 会对用户做一些限制，所以我们要先模拟登录） 其次保持会话连接（可以考虑 cookie 保存，这里采用的是 requests 的 session 会话连接） （基于图片网站，可能是动态加载，那么需要分析接口或者是 selenium 模拟） 最后才进行网页内容分析，然后抓取保存下来 3、登录模拟实现流程 一、查找登录接口 第一次找关于登录接口的时候，一个login都没看到，只看到一个 www.pixiv.net ,可惜是get请求的页面。 在拜读了 Chrome使用技巧 、Chrome开发者工具使用小技巧 后，算是对 chrome 的调试工具有个大概了解的印象，知道了 preserve log 勾选后，可以保留网络日志，于是发现了真正的登录请求 分析参数 password：个人密码 pixiv_id：个人id post_key：不明字符串 source：pc即电脑端（截图没截全，把return_to漏掉了。。。） return_to：是登录成功后跳转的页面，这个可以自己填，貌似默认是 https://www.pixiv.net/ 那么接下来就是找post_key了 首先pixiv非常友好，所以应该不是js加密，而是在页面中随机生成的。 12345# 其次在点击登录的时候就跳转 url1 ↓# url1 = https://accounts.pixiv.net/login?lang=zh&amp;source=pc&amp;view_type=page&amp;ref=wwwtop_accounts_index# 但是登录请求的 url 是 url2 ↓# url2 = https://accounts.pixiv.net/api/login?lang=zh# 所以猜想 post_key 应该是在前者中生成的。 F12 打开，在 Elements 中 Ctrl + F 查看 post_key 接下来用 BeautifulSoup 匹配 12self.post_key = post_key_soup.find('input') ['value'] # 因为是第一个input标签，而find返回的是第一个符合要求的结果 接着向 url2 发去 post 请求 12345678data = &#123; 'pixiv_id': self.pixiv_id, 'password': self.password, 'return_to': self.return_to, 'post_key': self.post_key&#125;rep = se.post(self.login_url, data=data, headers=self.headers,verify=False)# login_url是上面的 url2# 我这里 return_to 写的是个人关注画师的那个页面的 url 登录代码 12345678910111213141516import requestsfrom bs4 import BeautifulSoupse = requests.session()def login(self): post_key_html = self.request(self.base_url) # base_url 是上面的 url1 post_key_soup = BeautifulSoup(post_key_html.text, 'lxml') self.post_key = post_key_soup.find('input')['value'] print(self.post_key) #捕获postkey data = &#123; 'pixiv_id': self.pixiv_id, 'password': self.password, 'return_to': self.return_to, 'post_key': self.post_key&#125; rep = se.post(self.login_url, data=data, headers=self.headers,verify=False) # login_url 是上面的 url2 print('登陆成功') 顺便吐槽下 HTTPS 的证书报警问题 12from requests.packages.urllib3.exceptions import InsecureRequestWarning #强制取消警告requests.packages.urllib3.disable_warnings(InsecureRequestWarning) 4、最后 先到这吧，明天继续写解析关注画师页面（页数），寻找数据接口，单图动图多图下载估计写不到了 へ(￣ ￣;へ)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>lxml</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Next主题集成 algolia 站内搜索插件]]></title>
    <url>%2Fblog%2F2019%2F03%2F26%2Falgolia%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：Next主题集成algolia 一、简述 今天添加并完善了下blog的站内搜索（既然next有集成，为何不用呢？） 主要是参考了几篇超级详细的文章 大赞：Hexo+Next集成Algolia搜索、知乎、hexo-algolia 二、如何更新？123456# 删除 public 文件夹hexo clean# 搜集站点的内容并通过 API 发送给 Algoliahexo algolia# 如遇到 ERROR [Algolia] Please set an `HEXO_ALGOLIA_INDEXING_KEY` environment variable to enable content indexing.# export HEXO_ALGOLIA_INDEXING_KEY=[你的API Key] 三、问题枪毙名单1. Not enough rights to update an object near 解决方法：修改Algolia的ACL访问控制列表 将ACL修改为以上所示，文章里的ACL和现在的界面不一样，不知道是我用得少的原因(雾)，找了几分钟左右。 2. Please provide an Algolia index name in your hexo _config.yml flle 解决方法：修改index名称 index名称就是在以下这个界面输入的那个index name 3. ERROR [Algolia] Please set an HEXO_ALGOLIA_INDEXING_KEY environment variable to enable content indexing. 这个通常是在hexo algolia的时候出现的问题 其实在上面的文章也有说到，这里简单说一下 解决方法： 1export HEXO_ALGOLIA_INDEXING_KEY=[你的API Key] API Key 是 Search-Only API key]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lxml&Xpath]]></title>
    <url>%2Fblog%2F2019%2F03%2F13%2Fxpath%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：Xpath语法，lxml解析html 一、什么是Xpath? XPath 是 XML 路径语言，主要是在 XML 和 HTML 文档中查找我们想要的信息的语言。 XML 和 HTML 一样也是标记语言，但是 XML 用来传输和存储数据，而 HTML 用来显示数据 二、Xpath工具 Google：Xpath Helper （Google 插件可到下载Crx4Chrom(英文)、插件网、Chrome插件网 下载） Firefox：Try Xpath 每个浏览器一般在应用中心或拓展里都可以下载 Xpath Helper 界面 三、Xpath语法1、路径表达式语法、相对/绝对路径 表达式 路径表达式及描述 节点名称 bookstore，选取 bookstore 下的所有子节点(标签) / /bookstore，从根节点下选取所有 bookstore 节点(子元素) // //bookstore，从全局节点中选择 bookstore 节点 @ //div[@price=‘a’]，选择所有 price 属性为 a 的 div . ./input，选择当前节点下的 input 2、谓语html 节点中第一个节点为 1，第二个为 2（需要区分） 表达式 描述 //ul/li[1] 选择 ul 下的第一个 li //ul/li[last()-0] 选择 ul 下的最后一个 li //ul/li[last()-1] 选择 ul 下的倒数第二个 li //ul/li[position()&lt;4] 选择 ul 下前面的 3 个子元素 //ul/li[position()&gt;1] 选择第二个到最后的所有子元素 //li[position()&gt;1] [position()&lt;11] 在(2,+∞)中选择前十个 text() 获取函数文本 @class 获取标签的class 通配符 * /bookstore/*，通配符，匹配 bookstore 下的所有子元素 @* //div[@*]，选择所有带有属性的 div 运算符 “|“ —&gt; //title | //ul[@class=‘item_con_list’]，选择 title 和对应的 ul 四、使用lxml&amp;xpath解析html123456html1 = etree.parse(index.html) # 可以通过读取html文件的方式html2 = etree.HTML(text) # 也可以将字符串解析为HTML文档result1 = etree.tostring(html2) # 将字符串序列化成HTML文档,会自动补全result2 = html2.xpath('表达式') # 使用xpath语法 五、Example 以腾讯招聘网为例 我们要获取到职位名称、职位类别、人数、地点和发布时间等内容 1//table//tr 选择到了13个子元素，分别是表头，翻页和底部其他招聘 那么可以往上找父元素，扩大范围 1//table[@class='tablelist']//tr[@class='even'] | //table[@class='tablelist']//tr[@class='odd'] 使用以上表达式，避开表头和翻页 上述表达式虽然精确但是有点冗长，鉴于网站规律性，可以采用以下表达式 1//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11] 上面的表达式写在程序里不加text()来取的话，会返回类似 的结果。 123456789101112from lxml import etreeimport requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'&#125;url = 'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'html = requests.get(url=url,headers=headers)# html.text未经过编码的字符串，unicode字符串etree_obj = etree.HTML(html.text) # HTML解析的是字符串，所以html.textresult1 = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()")result2 = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]")print(result1)print(result2) 结果可以看到有许多的转义字符，比如：\r、\t 最后将代码处理一下，照此方法可以获取到腾讯招聘的所有职位信息(后面有个坑，比如职位类别是空数据的话，XPath匹配到会自动放弃) 1234567891011121314from lxml import etreeimport requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'&#125;url = 'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'html = requests.get(url=url,headers=headers)# html.text未经过编码的字符串，unicode字符串etree_obj = etree.HTML(html.text) # HTML解析的是字符串，所以html.textresult = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()")print(result)result2 = [x.strip() for x in result if x.strip() != '']print(result2)result3 = [print(result2[x],result2[x+1],result2[x+2],result2[x+3],result2[x+4]) for x in range(0,len(result2),5)]print(result3) 结果图 本篇代码Github地址：https://github.com/Coder-Sakura/exp/tree/master/xpath]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>Xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于git的操作的一些记录]]></title>
    <url>%2Fblog%2F2019%2F03%2F03%2Fgit-note%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：主要记录git的命令和一些git的知识 一、github介绍github(基佬站)是一个开源代码托管平台(其中当然也有私有项目)，也作为一个版本控制系统，让你对代码的版本控制更加简单，不用去担心代码写错了怎么办？有没有备份？专心自己的项目就好。 本文没有关于桌面版的git安装、环境变量配置的教程(安装配置的话百度有很多教程) 二、github功能 可以在上面找到许多开源项目、脚本甚至可以在上面找到一些课程 托管项目。只要连上互联网就可以同步到自己的项目代码或多人跟进项目 利用github和一些开源的博客系统可以搭建个人博客(本博客是hexo+github搭建的) 三、git命令 设置用户名和邮箱，不设置会报“please tell me who you are.”，–global参数表示全局 123git config --global user.name "Your Name" git config --global user.email "email@example.com"git config --list # 检查设置 初始化本地文件夹为git仓库（会生成.git隐藏文件，主要是用于版本控制） 1git init 本地版本管理 123456789101112git add ./[name] # 跟踪文件进入暂存区，.表示当前目录所有文件，也可以指定文件git status # 命令用于显示工作目录和暂存区的状态git commit -m '提交说明' # 将暂存区里的改动给提交到本地的版本库git log --pretty=oneline # 查看最近到最远的提交日志，oneline表示每条输出一行# $ git log --pretty=oneline# f3e98b7f4495c78bf98f2661fad2ae745cd60b63 (HEAD -&gt; master, origin/master) proxy# f3e9....这串就是这次提交的版本号git reset --hard [版本号] # 回退/前进到某个指定版本，版本号可以在git log中找到# 也有快捷的回退命令git reset --hard HEAD^ # 回退到上个版本git reset --hard HEAD^^ # 回退到上上个版本git reset --hard HEAD~100 # 回退到上100个版本 将本地文件提交到github 123456789101112git init git add ./(name) git commit -m 'message'git remote add origin [github仓库地址] # 如果出现错误：fatal: remote origin already exists# 执行 git remote rm origin #删除分支# 再执行 git remote add origin [github仓库地址] #再添加 git push origin master # 推送到github仓库# 如果出现failed to push som refs to…….# 需要将github仓库的文件同步下来先# git pull origin master # pull拉文件下来# 再执行 git push origin master # push推文件上去 四、2019/03/30删除远程仓库的文件，并上传新的文件 12345678910111213# 新建一个文件夹git initgit remote add origin [仓库地址] # git remote rm origingit pull origin master# 然后在本地删除文件git add *git commit -m "del all"git push origin master# 放入新的文件git add *git commit -m "create new file"git push origin master]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫[代理]]]></title>
    <url>%2Fblog%2F2019%2F02%2F28%2Fproxy%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：代理原理作用，requests设置代理方法以及爬取免费代理的脚本实例 一、代理原理 根据自己理解解读： 客户端设置了代理信息后，客户端向对应的代理站点发出请求（向xxx网站发起请求） 代理站点收到请求之后，就会执行对应的响应动作（执行动作） 代理站点获得xxx网站的响应（得到站点响应） 代理站点根据客户端要求返回对应信息（客户端要求返回Source code，则返回Source code） 二、代理作用 突破自身ip访问限制，比如访问国外站点 爬取对ip访问频率有一定限制的站点 提高访问速度 隐藏真实ip 三、代理网站免费代理ip列表： 含国外ip 方法SEO顾问，89代理，小幻http代理，云代理 不含 西刺，快代理 付费代理尚未了解，此处留空 四、requests设置代理方法requests中有预设好的参数接收代理信息 proxies，这个参数接收的是一个字典对象 因为不知道访问的网站使用的是http协议还是https协议，所以proxies最好2种都有设置 12345proxies = &#123; 'http':92.255.255.78:54628, 'https':92.255.255.78:54628&#125;resp = requests.get(url=url1,headers=headers,proxies=proxies) 五、脚本示例github地址：https://github.com/Coder-Sakura/exp/tree/master/seo_ip 本来我是打算用89代理的api接口，但是测试之后发现可靠性有点低，并且外网ip比较少，所以转用SEO （本次抓取代理ip主要是用在我自己做 pixiv 的小项目上，爬取关注画师的所有作品和自己的收藏作品，后续会整理出来，初学爬虫，有错还请指正） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import requestsimport randomimport timefrom lxml import etreefrom bs4 import BeautifulSoupfrom requests.packages.urllib3.exceptions import InsecureRequestWarning # 用于强制取消警告from requests.adapters import HTTPAdapter # 用于强制取消警告requests.packages.urllib3.disable_warnings(InsecureRequestWarning) # 强制取消警告class seo_ip(): def __init__(self): self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'&#125; self.agent_ip_list = [] def Agent(self,ip_agent_url): html = requests.get(url=ip_agent_url,headers=self.headers,verify=False,timeout=5) html_soup = BeautifulSoup(html.text, 'lxml') # 去除第一个和前25个，26-50为国外ip ip_list = html_soup.find('tbody').find_all('tr')[26:] items = [] print('搜索完成,代理信息如下:') for item in ip_list: ip_port = list(item)[0].get_text() + ':' +list(item)[1].get_text() # list(ip_port)[0]为ip,[1]为端口,[2]响应时间,[3]位置,[4]最后验证时间 print('ip: %s ,响应时间: %ss ,ip位置: %s' % (ip_port,list(item)[2].get_text(),list(item)[3].get_text())) items.append(ip_port) #存储爬取到的ip(需要添加) return items def judge(self,items): # 检验ip活性 # https://ip.seofangfa.com/ print('正在进行代理池ip活性检测......') for item in items: try: proxy = &#123; 'http':item, 'https':item &#125; # 遍历时，利用百度，设定timeout，未响应则断开连接 judge_url = 'https://www.baidu.com/' response = requests.get(url=judge_url,headers=self.headers,proxies=proxy,verify=False,timeout=5) self.agent_ip_list.append(item) print(item,'可用...') except: print(item,'不可用...') print('代理池ip活性检测完毕...\n代理池总量:',len(self.agent_ip_list),'\n代理池:',self.agent_ip_list) def work(self): ip_agent_url = 'https://ip.seofangfa.com/' items = self.Agent(ip_agent_url) self.judge(items)seo_ip = seo_ip()seo_ip.work() 没有导入这2个库的话，会因为ssl证书而出现警告，如图： from requests.packages.urllib3.exceptions import InsecureRequestWarning from requests.adapters import HTTPAdapter 六、附图 最后附上运行图]]></content>
      <categories>
        <category>proxy</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python第三方库安装[pip、whl]]]></title>
    <url>%2Fblog%2F2019%2F02%2F12%2F212%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：python第三方库安装 一、pip安装 pip3 install [库名] 或 pip install [库名] [2019.02.28更新] 针对 ‘pip’ 不是内部命令，也不是可运行的程序的情况： 原因：环境变量 Path 未配置完成 找到 python 的安装目录，将 python 的安装目录 和 Python安装目录\Scripts 添加到 环境变量 Path 中即可； 环境变量Path：计算机属性 -&gt; 高级系统设置 -&gt; 高级 -&gt; 环境变量 二、whl安装点我可以将内容伸缩哦~ ٩(๑&gt;◡&lt;๑)۶ 如果pip安装不行，可以考虑whl安装(轮子大法好！) Python常用库whl文件下载 如何知道本机安装的python支持哪个版本的轮子? 首先要知道系统是多少位的？(在cmd中输入)1systeminfo | findstr "系统类型" 结果：x64-based PC = (64位 AMD64) 12import pip._internalprint(pip._internal.pep425tags.get_supported()) X86-based PC = (32位 WIN32) 12import pipprint(pip.pep425tags.get_supported()) 选择对应的轮子 输入上面的代码后，会返回一个list，list里面就是当前系统支持的whl版本 比如 ‘cp37’, ‘cp37m’, ‘win_amd64’，cp37对应的是python3.7版本; cp37m 对应的是依赖于python3.7应用程序二进制接口; win_amd64对应的是64位系统编译的。 打开上面的网址，ctrl + F ，这里使用 mysqlclient 作为示范 根据刚刚的结果，下载以下版本的轮子即可。 mysqlclient‑1.4.2‑cp37‑cp37m‑win_amd64.whl ​ 库名 - 版本号 - 对应python版本 - 依赖 - 系统位数 安装轮子 进入到轮子目录，cmd打开，pip install [名字].whl 即可 [2019.02.28更新] 针对 pip install [名字].whl 安装不成功的情况 可以将whl文件的后缀名.whl更改为.zip，然后解压 在解压目录下进行 python setup.py install 运行安装[通常whl文件解压后都有setup.py] 对于没有 setup.py 的，直接将解压目录放入libs文件夹中 whl文件是已经编译好的文件，作用主要是为了方便我们进行 python 的第三方库安装和使用 三、Anaconda​ Anaconda包括Conda、Python以及一大堆安装好的科学包和依赖项。(Conda是一个开源的包和环境的管理器) ​ 从 Anaconda官网 下载，图形化安装，十分简单，而且网上的教程也多。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>第三方库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2019%2F02%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
