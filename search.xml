<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[lxml&Xpath]]></title>
    <url>%2Fblog%2F2019%2F03%2F13%2Fxpath%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：Xpath语法，lxml解析html 一、什么是Xpath? XPath 是 XML 路径语言，主要是在 XML 和 HTML 文档中查找我们想要的信息的语言。 XML 和 HTML 一样也是标记语言，但是 XML 用来传输和存储数据，而 HTML 用来显示数据 二、Xpath工具 Google：Xpath Helper （Google 插件可到下载Crx4Chrom(英文)、插件网、Chrome插件网 下载） Firefox：Try Xpath 每个浏览器一般在应用中心或拓展里都可以下载 Xpath Helper 界面 三、Xpath语法1、路径表达式语法、相对/绝对路径 表达式 路径表达式及描述 节点名称 bookstore，选取 bookstore 下的所有子节点(标签) / /bookstore，从根节点下选取所有 bookstore 节点(子元素) // //bookstore，从全局节点中选择 bookstore 节点 @ //div[@price=‘a’]，选择所有 price 属性为 a 的 div . ./input，选择当前节点下的 input 2、谓语html 节点中第一个节点为 1，第二个为 2（需要区分） 表达式 描述 //ul/li[1] 选择 ul 下的第一个 li //ul/li[last()-0] 选择 ul 下的最后一个 li //ul/li[last()-1] 选择 ul 下的倒数第二个 li //ul/li[position()&lt;4] 选择 ul 下前面的 3 个子元素 //ul/li[position()&gt;1] 选择第二个到最后的所有子元素 //li[position()&gt;1] [position()&lt;11] 在(2,+∞)中选择前十个 text() 获取函数文本 @class 获取标签的class 通配符 * /bookstore/*，通配符，匹配 bookstore 下的所有子元素 @* //div[@*]，选择所有带有属性的 div 运算符 “|“ —&gt; //title | //ul[@class=‘item_con_list’]，选择 title 和对应的 ul 四、使用lxml&amp;xpath解析html123456html1 = etree.parse(index.html) # 可以通过读取html文件的方式html2 = etree.HTML(text) # 也可以将字符串解析为HTML文档result1 = etree.tostring(html2) # 将字符串序列化成HTML文档,会自动补全result2 = html2.xpath('表达式') # 使用xpath语法 五、Example 以腾讯招聘网为例 我们要获取到职位名称、职位类别、人数、地点和发布时间等内容 1//table//tr 选择到了13个子元素，分别是表头，翻页和底部其他招聘 那么可以往上找父元素，扩大范围 1//table[@class='tablelist']//tr[@class='even'] | //table[@class='tablelist']//tr[@class='odd'] 使用以上表达式，避开表头和翻页 上述表达式虽然精确但是有点冗长，鉴于网站规律性，可以采用以下表达式 1//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11] 上面的表达式写在程序里不加text()来取的话，会返回类似 的结果。 123456789101112from lxml import etreeimport requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'&#125;url = 'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'html = requests.get(url=url,headers=headers)# html.text未经过编码的字符串，unicode字符串etree_obj = etree.HTML(html.text) # HTML解析的是字符串，所以html.textresult1 = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()")result2 = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]")print(result1)print(result2) 结果可以看到有许多的转义字符，比如：\r、\t 最后将代码处理一下，照此方法可以获取到腾讯招聘的所有职位信息(后面有个坑，比如职位类别是空数据的话，XPath匹配到会自动放弃) 1234567891011121314from lxml import etreeimport requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'&#125;url = 'https://hr.tencent.com/position.php?lid=2218&amp;start=0#a'html = requests.get(url=url,headers=headers)# html.text未经过编码的字符串，unicode字符串etree_obj = etree.HTML(html.text) # HTML解析的是字符串，所以html.textresult = etree_obj.xpath("//table[@class='tablelist']//tr[position()&gt;1][position()&lt;11]//text()")print(result)result2 = [x.strip() for x in result if x.strip() != '']print(result2)result3 = [print(result2[x],result2[x+1],result2[x+2],result2[x+3],result2[x+4]) for x in range(0,len(result2),5)]print(result3) 本篇代码Github地址：https://github.com/Coder-Sakura/exp/tree/master/xpath]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>Xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于git的操作的一些记录]]></title>
    <url>%2Fblog%2F2019%2F03%2F03%2Fgit-note%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：主要记录git的命令和一些git的知识 一、github介绍github(基佬站)是一个开源代码托管平台(其中当然也有私有项目)，也作为一个版本控制系统，让你对代码的版本控制更加简单，不用去担心代码写错了怎么办？有没有备份？专心自己的项目就好。 本文没有关于桌面版的git安装、环境变量配置的教程(安装配置的话百度有很多教程) 二、github功能 可以在上面找到许多开源项目、脚本甚至可以在上面找到一些课程 托管项目。只要连上互联网就可以同步到自己的项目代码或多人跟进项目 利用github和一些开源的博客系统可以搭建个人博客(本博客是hexo+github搭建的) 三、git命令 设置用户名和邮箱，不设置会报“please tell me who you are.”，–global参数表示全局 123git config --global user.name "Your Name" git config --global user.email "email@example.com"git config --list # 检查设置 初始化本地文件夹为git仓库（会生成.git隐藏文件，主要是用于版本控制） 1git init 本地版本管理 123456789101112git add ./[name] # 跟踪文件进入暂存区，.表示当前目录所有文件，也可以指定文件git status # 命令用于显示工作目录和暂存区的状态git commit -m '提交说明' # 将暂存区里的改动给提交到本地的版本库git log --pretty=oneline # 查看最近到最远的提交日志，oneline表示每条输出一行# $ git log --pretty=oneline# f3e98b7f4495c78bf98f2661fad2ae745cd60b63 (HEAD -&gt; master, origin/master) proxy# f3e9....这串就是这次提交的版本号git reset --hard [版本号] # 回退/前进到某个指定版本，版本号可以在git log中找到# 也有快捷的回退命令git reset --hard HEAD^ # 回退到上个版本git reset --hard HEAD^^ # 回退到上上个版本git reset --hard HEAD~100 # 回退到上100个版本 将本地文件提交到github 123456789101112git init git add ./(name) git commit -m 'message'git remote add origin [github仓库地址] # 如果出现错误：fatal: remote origin already exists# 执行 git remote rm origin #删除分支# 再执行 git remote add origin [github仓库地址] #再添加 git push origin master # 推送到github仓库# 如果出现failed to push som refs to…….# 需要将github仓库的文件同步下来先# git pull origin master # pull拉文件下来# 再执行 git push origin master # push推文件上去]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫[代理]]]></title>
    <url>%2Fblog%2F2019%2F02%2F28%2Fproxy%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：代理原理作用，requests设置代理方法以及爬取免费代理的脚本实例 一、代理原理 根据自己理解解读： 客户端设置了代理信息后，客户端向对应的代理站点发出请求（向xxx网站发起请求） 代理站点收到请求之后，就会执行对应的响应动作（执行动作） 代理站点获得xxx网站的响应（得到站点响应） 代理站点根据客户端要求返回对应信息（客户端要求返回Source code，则返回Source code） 二、代理作用 突破自身ip访问限制，比如访问国外站点 爬取对ip访问频率有一定限制的站点 提高访问速度 隐藏真实ip 三、代理网站免费代理ip列表： 含国外ip 方法SEO顾问，89代理，小幻http代理，云代理 不含 西刺，快代理 付费代理尚未了解，此处留空 四、requests设置代理方法requests中有预设好的参数接收代理信息 proxies，这个参数接收的是一个字典对象 因为不知道访问的网站使用的是http协议还是https协议，所以proxies最好2种都有设置 12345proxies = &#123; 'http':92.255.255.78:54628, 'https':92.255.255.78:54628&#125;resp = requests.get(url=url1,headers=headers,proxies=proxies) 五、脚本示例github地址：https://github.com/Coder-Sakura/proxy 本来我是打算用89代理的api接口，但是测试之后发现可靠性有点低，并且外网ip比较少，所以转用SEO （本次抓取代理ip主要是用在我自己做 pixiv 的小项目上，爬取关注画师的所有作品和自己的收藏作品，后续会整理出来，初学爬虫，有错还请指正） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import requestsimport randomimport timefrom lxml import etreefrom bs4 import BeautifulSoupfrom requests.packages.urllib3.exceptions import InsecureRequestWarning # 用于强制取消警告from requests.adapters import HTTPAdapter # 用于强制取消警告requests.packages.urllib3.disable_warnings(InsecureRequestWarning) # 强制取消警告class seo_ip(): def __init__(self): self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'&#125; self.agent_ip_list = [] def Agent(self,ip_agent_url): html = requests.get(url=ip_agent_url,headers=self.headers,verify=False,timeout=5) html_soup = BeautifulSoup(html.text, 'lxml') # 去除第一个和前25个，26-50为国外ip ip_list = html_soup.find('tbody').find_all('tr')[26:] items = [] print('搜索完成,代理信息如下:') for item in ip_list: ip_port = list(item)[0].get_text() + ':' +list(item)[1].get_text() # list(ip_port)[0]为ip,[1]为端口,[2]响应时间,[3]位置,[4]最后验证时间 print('ip: %s ,响应时间: %ss ,ip位置: %s' % (ip_port,list(item)[2].get_text(),list(item)[3].get_text())) items.append(ip_port) #存储爬取到的ip(需要添加) return items def judge(self,items): # 检验ip活性 # https://ip.seofangfa.com/ print('正在进行代理池ip活性检测......') for item in items: try: proxy = &#123; 'http':item, 'https':item &#125; # 遍历时，利用百度，设定timeout，未响应则断开连接 judge_url = 'https://www.baidu.com/' response = requests.get(url=judge_url,headers=self.headers,proxies=proxy,verify=False,timeout=5) self.agent_ip_list.append(item) print(item,'可用...') except: print(item,'不可用...') print('代理池ip活性检测完毕...\n代理池总量:',len(self.agent_ip_list),'\n代理池:',self.agent_ip_list) def work(self): ip_agent_url = 'https://ip.seofangfa.com/' items = self.Agent(ip_agent_url) self.judge(items)seo_ip = seo_ip()seo_ip.work() 没有导入这2个库的话，会因为ssl证书而出现警告，如图： from requests.packages.urllib3.exceptions import InsecureRequestWarning from requests.adapters import HTTPAdapter 六、附图 最后附上运行图]]></content>
      <categories>
        <category>proxy</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python第三方库安装[pip、whl]]]></title>
    <url>%2Fblog%2F2019%2F02%2F12%2F212%2F</url>
    <content type="text"><![CDATA[说在前面的话本篇概述：python第三方库安装 一、pip安装 pip3 install [库名] 或 pip install [库名] [2019.02.28更新] 针对 ‘pip’ 不是内部命令，也不是可运行的程序的情况： 原因：环境变量 Path 未配置完成 找到 python 的安装目录，将 python 的安装目录 和 Python安装目录\Scripts 添加到 环境变量 Path 中即可； 环境变量Path：计算机属性 -&gt; 高级系统设置 -&gt; 高级 -&gt; 环境变量 二、whl安装点我可以将内容伸缩哦~ ٩(๑&gt;◡&lt;๑)۶ 如果pip安装不行，可以考虑whl安装(轮子大法好！) Python常用库whl文件下载 如何知道本机安装的python支持哪个版本的轮子? 首先要知道系统是多少位的？(在cmd中输入)1systeminfo | findstr "系统类型" 结果：x64-based PC = (64位 AMD64) 12import pip._internalprint(pip._internal.pep425tags.get_supported()) X86-based PC = (32位 WIN32) 12import pipprint(pip.pep425tags.get_supported()) 选择对应的轮子 输入上面的代码后，会返回一个list，list里面就是当前系统支持的whl版本 比如 ‘cp37’, ‘cp37m’, ‘win_amd64’，cp37对应的是python3.7版本; cp37m 对应的是依赖于python3.7应用程序二进制接口; win_amd64对应的是64位系统编译的。 打开上面的网址，ctrl + F ，这里使用 mysqlclient 作为示范 根据刚刚的结果，下载以下版本的轮子即可。 mysqlclient‑1.4.2‑cp37‑cp37m‑win_amd64.whl ​ 库名 - 版本号 - 对应python版本 - 依赖 - 系统位数 安装轮子 进入到轮子目录，cmd打开，pip install [名字].whl 即可 [2019.02.28更新] 针对 pip install [名字].whl 安装不成功的情况 可以将whl文件的后缀名.whl更改为.zip，然后解压 在解压目录下进行 python setup.py install 运行安装[通常whl文件解压后都有setup.py] 对于没有 setup.py 的，直接将解压目录放入libs文件夹中 whl文件是已经编译好的文件，作用主要是为了方便我们进行 python 的第三方库安装和使用 三、Anaconda​ Anaconda包括Conda、Python以及一大堆安装好的科学包和依赖项。(Conda是一个开源的包和环境的管理器) ​ 从 Anaconda官网 下载，图形化安装，十分简单，而且网上的教程也多。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>第三方库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2019%2F02%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
